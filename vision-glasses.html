<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TuneTalk app</title>
    <link rel="stylesheet" href="styles.css">
    <style>
       
    </style>
</head>
<body>

<div class="background"></div>
<main id="tunetalk">

    <section class="header-hero">
    <div class="hero-text"> 
        <h1>Vision Glasses Overview</h1>
        <p>Research into a hypothetical design for AI glasses, designed to assist those with visual impairments</p>
    </section>

    <section class="content">

    <h2>Research</h2>
    <ul>
        <li><strong>Technical Research:</strong> Conducted secondary research on components required for AI-powered assistive glasses, including stereo cameras, YOLO-based object detection, DepthAI for triangulation, and cloud-based GPT models.</li>
        <li><strong>Market/Exemplar Research:</strong> Studied existing vision-assist devices such as Envision Glasses and Vuzix M4000. Key insight: most devices offload computation to mobile apps/cloud, but lack integrated depth estimation.</li>
    </ul>

    <h2>Design Goals</h2>
    <ul>
        <li>Develop a system that can identify and locate objects in 3D space using stereo vision and AI.</li>
        <li>Provide accessible audio feedback to visually impaired users through speech-to-text and text-to-speech integration.</li>
        <li>Ensure affordability, portability, and ergonomic design for everyday use.</li>
    </ul>

    <h2>Ideation</h2>
    <ul>
        <li><strong>Personas:</strong> Visually impaired individuals seeking real-time environmental awareness.</li>
        <li><strong>Scenarios:</strong> Example use case: a user asks, “Where is the chair?” and receives an audio description of the chair’s position.</li>
        <li>Brainstormed and outlined the end-to-end system pipeline: camera input → object detection/localization → GPT-based instruction → audio feedback.</li>
    </ul>

    <h2>Prototyping and Testing</h2>
    <ul>
        <li>Simulated YOLOv8 object detection using Ultralytics HUB for feasibility testing.</li>
        <li>Created diagrams and flowcharts to illustrate data pipelines (camera → smartphone app → cloud → TTS feedback).</li>
        <li>Outlined smartphone app architecture using React Native, focusing on STT, cloud GPT integration, and TTS output.</li>
    </ul>

    <h2>Insights and Learnings</h2>
    <ul>
        <li>Offloading AI inference to the cloud reduces hardware costs but introduces latency and reliance on connectivity.</li>
        <li>Stereo vision (DepthAI) adds unique functionality compared to current solutions like Envision or Vuzix, which lack position triangulation.</li>
        <li>Mobile OS features (native STT/TTS) can streamline app development and reduce external API dependencies.</li>
    </ul>

    <h2>Reflections and Post-Mortem</h2>
    <ul>
        <li><strong>What worked:</strong> Clear system architecture and strong feasibility from combining existing components (OAK-D, YOLO, GPT APIs).</li>
        <li><strong>What didn’t:</strong> Initial plan to run all computation locally (Jetson Orin Nano) proved too costly and less portable — pivoted to cloud approach.</li>
        <li><strong>Takeaway:</strong> Leveraging cloud AI and companion apps can make advanced assistive technology more affordable and scalable.</li>
    </ul>

    <h2>Giving Credit</h2>
    <ul>
        <li>Reference inspiration: Envision Glasses, Vuzix, and Ray-Ban Meta smart glasses.</li>
        <li>YOLOv8 by Ultralytics, DepthAI by Luxonis, OpenAI GPT API for generative feedback.</li>
    </ul>
    </section>

    <div class="pdf-container">
        <h2>Research Article</h2>
        <embed src="article.pdf" type="application/pdf" id="article">
    </div>


    </section>

</main>

</body>
</html>

